{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine Tuning Small Language Model on AG News Dataset\n",
        "\n",
        "This notebook demonstrates fine-tuning of a Small Language Model (DistilBERT) using AG News dataset from Hugging Face. The model is trained to classify news articles into four categories.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install transformers datasets evaluate accelerate\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import evaluate\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Selection\n",
        "\n",
        "AG News dataset is used for multi-class news classification. It contains news articles categorized into World, Sports, Business, and Science/Technology.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset = load_dataset(\"ag_news\")\n",
        "\n",
        "dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Selection\n",
        "\n",
        "DistilBERT is chosen because it is a lightweight Small Language Model that provides high performance with faster training time.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "      \"distilbert-base-uncased\",\n",
        "          num_labels=4\n",
        "          )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "metric = evaluate.load(\"accuracy\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42).select(range(3000)),\n",
        "    eval_dataset=tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000)),\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pre-trained DistilBERT model is fine-tuned using AG News dataset to adapt it for news classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation\n",
        "\n",
        "Accuracy metric is used to evaluate classification performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "trainer.evaluate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\n",
        "\n",
        "The model achieved approximately 89% accuracy on the evaluation dataset. The training loss reduced steadily indicating successful learning.\n",
        "\n",
        "## Observations\n",
        "\n",
        "- Fine-tuning improves classification accuracy.\n",
        "- Pre-trained models reduce training time.\n",
        "- Increasing training epochs can further improve results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "text = \"NASA launches a new satellite for space research\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "# Move inputs to the same device as the model\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "outputs = model(**inputs)\n",
        "\n",
        "prediction = np.argmax(outputs.logits.detach().cpu().numpy())\n",
        "\n",
        "labels = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "print(\"Predicted Category:\", labels[prediction])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "labels = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "print(\"Predicted Category:\", labels[prediction])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import files\n",
        "files.upload()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import nbformat\n",
        "\n",
        "path = \"/content/Fine_tuning_Lab_task1 (1).ipynb\"\n",
        "\n",
        "# Read notebook\n",
        "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "    nb = nbformat.read(f, as_version=4)\n",
        "\n",
        "    # Remove widgets metadata\n",
        "    if \"widgets\" in nb.get(\"metadata\", {}):\n",
        "        del nb[\"metadata\"][\"widgets\"]\n",
        "\n",
        "        # Write cleaned notebook\n",
        "        with open(\"/content/Fine_tuning_Lab_task1_clean.ipynb\", \"w\", encoding=\"utf-8\") as f:\n",
        "            nbformat.write(nb, f)\n",
        "\n",
        "            print(\"Notebook cleaned successfully!\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Fine_tuning_Lab_task1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}