{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTAxRzByM-Zj"
      },
      "outputs": [],
      "source": [
        "!pip install datasets sentence-transformers faiss-cpu transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset**"
      ],
      "metadata": {
        "id": "lCKyf9QWNu3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"ag_news\", split=\"train[:500]\")\n",
        "\n",
        "documents = [doc[\"text\"] for doc in dataset]"
      ],
      "metadata": {
        "id": "l6iOB6lcNCMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunking**"
      ],
      "metadata": {
        "id": "J-eFc8hxN7gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, chunk_size=500, overlap=100):\n",
        "    chunks = []\n",
        "    for i in range(0, len(text), chunk_size - overlap):\n",
        "        chunks.append(text[i:i + chunk_size])\n",
        "    return chunks\n",
        "\n",
        "all_chunks = []\n",
        "for doc in documents:\n",
        "    all_chunks.extend(chunk_text(doc))"
      ],
      "metadata": {
        "id": "rl6R3NV1NEU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Embeddings**"
      ],
      "metadata": {
        "id": "XCvTJdioN_vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "embeddings = embed_model.encode(all_chunks)"
      ],
      "metadata": {
        "id": "rXT1wfoJNHEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Store in FAISS**"
      ],
      "metadata": {
        "id": "p1ZH2tTXOECM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(np.array(embeddings))"
      ],
      "metadata": {
        "id": "HMnF8UuPNJFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retrieval Function**"
      ],
      "metadata": {
        "id": "RNDHi7-8OJFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query, top_k=3):\n",
        "    query_embedding = embed_model.encode([query])\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    return [all_chunks[i] for i in indices[0]]"
      ],
      "metadata": {
        "id": "I7nxWk_sNMyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load LLM**"
      ],
      "metadata": {
        "id": "9oJL1xqKOOCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = \"google/flan-t5-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "J7-tObSRQ-wY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAG Answer Function**"
      ],
      "metadata": {
        "id": "irhOfoWVOXXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_answer(query):\n",
        "    retrieved_docs = retrieve(query)\n",
        "    context = \" \".join(retrieved_docs)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Answer the question in 3-4 clear sentences based on the context.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100\n",
        "    )\n",
        "\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "zBF_I2JDNNsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Queries**"
      ],
      "metadata": {
        "id": "Hwb-9Bi4Odi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_answer(\"What is machine learning?\"))\n"
      ],
      "metadata": {
        "id": "VNNcadfANS4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_answer(\"Explain neural networks.\"))\n"
      ],
      "metadata": {
        "id": "7JVLVu9IO4R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_answer(\"What are research challenges in AI?\"))"
      ],
      "metadata": {
        "id": "7EVEjDsgPBRD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}